---
title: "End-to-End Model Generation with Large Language Models for Adaptive IoT Application Deployment"
collection: publications
category: conferences
permalink: /publication/2025-01-17-nag
excerpt: 'Zhenyu Wen, Feng Jintao, Nanjie Yao, Di Wu, Cong Wang, Shibo He'
date: 2025-01-17
venue: 'IEEE/ACM International Conference on Software Engineering (IEEE/ACM ICSE), 2026'
# paperurl: 'https://arxiv.org/pdf/2412.03103'
# citation: 'Your Name, You. (2024). &quot;Paper Title Number 3.&quot; <i>GitHub Journal of Bugs</i>. 1(3).'
---
![overview](../images/paper_overview/overview_icse.png)

The pipeline of using LLM-AMG to generate deployable deep neural networks while meeting the latency constraint. 

# Abstract
The deployment of AI-powered applications on resource-constrained edge devices presents a significant software engineering challenge. While Pruning and Neural Architecture Search (NAS) have shown promise in optimizing model efficiency, their application in edge device deployment is often limited by low levels of automation. In this paper, we introduce LLM-based Adaptive Model Generation (LAMDA), a novel framework that tackle the challenge of adaptive AI deployment as an automated model generation problem. LAMDA empowers an LLM to perform end-to-end design, refinement, and optimization of DNNs to meet the specific hardware constraints. Our approach makes two primary contributions. First, we introduce a serialization technique that transforms complex DNN computation graphs into a structured textual representation, effectively creating a Domain-Specific Language (DSL) that makes model architectures comprehensible and manipulable by an LLM. Further, to ground the generation process in real-world hardware constraints, we integrate a feedback-driven optimization loop. This loop leverages an empirical performance model, trained to correlate architectural patterns with on-device latency, enabling the LLM to reason about and optimize for non-functional requirements. To mitigate architectural ``hallucinations'', we incorporate context management and validation to ensure valid generation. We evaluate LAMDA through extensive experiments on public benchmarks and real-world edge devices. The results demonstrate that our framework can autonomously generate and adapt DNNs that satisfy deployment-specific accuracy and latency constraints, significantly advancing the state-of-the-art in automated software adaptation for the AI-enabled edge devices.